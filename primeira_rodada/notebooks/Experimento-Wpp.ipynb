{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac1c18da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji, re, string, time, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import randint\n",
    "import pickle\n",
    "\n",
    "#nlp\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "\n",
    "#dataviz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "#models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.ensemble  import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#data balancing\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "sns.set(style=\"darkgrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0ee5f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'results_wpp/viral-15-words/ml/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = 'viral-15-words'\n",
    "path_dir = 'results_wpp/' + subset + '/ml/'\n",
    "path_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b84393ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_processado.csv\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../data_wpp' #+ '/vis_processed_texts.p'\n",
    "preprocessed = False # the texts were already pre-processed\n",
    "processed_texts_filename = 'processed_texts-'+subset+'.p'\n",
    "for filename in os.listdir(data_dir):\n",
    "    print(filename)\n",
    "    if filename == processed_texts_filename:\n",
    "        preprocessed = True\n",
    "preprocessed \n",
    "\n",
    "filepath = '../data_wpp/df_processado.csv'\n",
    "df = pd.read_csv(filepath)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd071861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text_content</th>\n",
       "      <th>Contador</th>\n",
       "      <th>Golpe</th>\n",
       "      <th>processed_text_content</th>\n",
       "      <th>n_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ü•≥ *EMPR√âSTIMO SIMPLES E R√ÅPIDO SEM BUROCRACIA*...</td>\n",
       "      <td>175</td>\n",
       "      <td>1</td>\n",
       "      <td>rosto_festivo * emprestimo simples rapido buro...</td>\n",
       "      <td>1286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>üí≤üî• *URGENTE* üî•üí≤\\nüíØ *NOSSO GRUPO DE REFER√äNCIA ...</td>\n",
       "      <td>158</td>\n",
       "      <td>1</td>\n",
       "      <td>cifr fogo * urgente * fogo cifr \\n cem_ponto *...</td>\n",
       "      <td>1639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>E AGORA XAND√ÉO ? \\nVai mandar prender o Genera...</td>\n",
       "      <td>116</td>\n",
       "      <td>1</td>\n",
       "      <td>agora xandao ? \\n ir mandar prender general Gi...</td>\n",
       "      <td>4463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>*ATAQUE DE LULA AO ‚ÄúMEI‚Äù CAUSA REVOLTA E PODE ...</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>* ataque Lula \"mei\" causar revoltar poder deci...</td>\n",
       "      <td>401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>A ju√≠za substituta da 6¬™ Vara Criminal de Lond...</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>juiza substituta a var criminal londrino pr dr...</td>\n",
       "      <td>2528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  \\\n",
       "0             0           0   \n",
       "1             1           1   \n",
       "2             2           2   \n",
       "3             3           3   \n",
       "4             4           4   \n",
       "\n",
       "                                        text_content  Contador  Golpe  \\\n",
       "0  ü•≥ *EMPR√âSTIMO SIMPLES E R√ÅPIDO SEM BUROCRACIA*...       175      1   \n",
       "1  üí≤üî• *URGENTE* üî•üí≤\\nüíØ *NOSSO GRUPO DE REFER√äNCIA ...       158      1   \n",
       "2  E AGORA XAND√ÉO ? \\nVai mandar prender o Genera...       116      1   \n",
       "3  *ATAQUE DE LULA AO ‚ÄúMEI‚Äù CAUSA REVOLTA E PODE ...       114      0   \n",
       "4  A ju√≠za substituta da 6¬™ Vara Criminal de Lond...        68      0   \n",
       "\n",
       "                              processed_text_content  n_words  \n",
       "0  rosto_festivo * emprestimo simples rapido buro...     1286  \n",
       "1  cifr fogo * urgente * fogo cifr \\n cem_ponto *...     1639  \n",
       "2  agora xandao ? \\n ir mandar prender general Gi...     4463  \n",
       "3  * ataque Lula \"mei\" causar revoltar poder deci...      401  \n",
       "4  juiza substituta a var criminal londrino pr dr...     2528  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1abba4",
   "metadata": {},
   "source": [
    "# Fun√ß√µes que ser√£o usadas para processamento de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f06ad9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "unicode_emoji = {}\n",
    "for key, value in emoji.EMOJI_DATA.items():\n",
    "    try:\n",
    "        unicode_emoji[key] = value['pt']\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "#emojis and punctuation\n",
    "emojis_list = list(unicode_emoji)\n",
    "punct = list(string.punctuation)\n",
    "emojis_punct = emojis_list + punct\n",
    "\n",
    "def processEmojisPunctuation(text, remove_punct = True):\n",
    "    '''\n",
    "    Put spaces between emojis. Removes punctuation.\n",
    "    '''\n",
    "    #get all unique chars\n",
    "    chars = set(text)\n",
    "    #for each unique char in text, do:\n",
    "    for c in chars:\n",
    "        #remove punctuation\n",
    "        if remove_punct:\n",
    "            if c in emojis_list:\n",
    "                text = text.replace(c, ' ' + c + ' ')\n",
    "            if c in punct:\n",
    "                text = text.replace(c, ' ')\n",
    "\n",
    "        #put spaces between punctuation\n",
    "        else:\n",
    "            if c in emojis_punct:\n",
    "                text = text.replace(c, ' ' + c + ' ')          \n",
    "\n",
    "    text = text.replace('  ', ' ')\n",
    "    return text\n",
    "\n",
    "#stop words removal\n",
    "stop_words = list(stopwords.words('portuguese'))\n",
    "new_stopwords = ['a√≠','pra','v√£o','vou','onde','l√°','aqui',\n",
    "                 't√°','pode','pois','so','deu','agora','todo',\n",
    "                 'nao','ja','vc', 'bom', 'ai','kkk','kkkk','ta', 'voce', 'alguem', 'ne', 'pq',\n",
    "                 'cara','to','mim','la','vcs','tbm', 'tudo']\n",
    "stop_words = stop_words + new_stopwords\n",
    "final_stop_words = []\n",
    "for sw in stop_words:\n",
    "    sw = ' '+ sw + ' '\n",
    "    final_stop_words.append(sw)\n",
    "\n",
    "def removeStopwords(text):\n",
    "    for sw in final_stop_words:\n",
    "        text = text.replace(sw,' ')\n",
    "    text = text.replace('  ',' ')\n",
    "    return text\n",
    "\n",
    "#lemmatization\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "def lemmatization(text):\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.text != token.lemma_:\n",
    "            text = text.replace(token.text, token.lemma_)\n",
    "    return text\n",
    "\n",
    "\n",
    "def domainUrl(text):\n",
    "    '''\n",
    "    Substitutes an URL in a text for the domain of this URL\n",
    "    Input: an string\n",
    "    Output: the string with the modified URL\n",
    "    '''    \n",
    "    if 'http' in text:\n",
    "        re_url = '[^\\s]*https*://[^\\s]*'\n",
    "        matches = re.findall(re_url, text, flags=re.IGNORECASE)\n",
    "        for m in matches:\n",
    "            domain = m.split('//')\n",
    "            domain = domain[1].split('/')[0]\n",
    "            text = re.sub(re_url, domain, text, 1)\n",
    "        return text\n",
    "    else:\n",
    "        return text \n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower().strip()\n",
    "    text = domainUrl(text)\n",
    "    text = processEmojisPunctuation(text)\n",
    "    text = removeStopwords(text)\n",
    "    text = lemmatization(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b33d19",
   "metadata": {},
   "source": [
    "# Definir quais experimentos ser√£o feitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76407ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = ['ml-tfidf-random_oversampling',\n",
    " 'ml-tfidf-processed-random_oversampling',\n",
    " 'ml-tfidf-unibi_gram-random_oversampling',\n",
    " 'ml-tfidf-unibitri_gram-random_oversampling',\n",
    " 'ml-tfidf-unibitri_gram-processed-random_oversampling',\n",
    " 'ml-bow-random_oversampling',\n",
    " 'ml-bow-random_oversampling-processed',\n",
    " 'ml-bow-unibitri_gram-random_oversampling',\n",
    " 'ml-bow-unibitri_gram-processed-random_oversampling']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bdc1ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments1 = ['ml-tfidf-unibi_gram-processed-random_oversampling',\n",
    "               'ml-bow-unibi_gram-processed-random_oversampling',\n",
    "               'ml-bow-unibi_gram-random_oversampling'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15f36bf",
   "metadata": {},
   "source": [
    "# Definindo a fun√ß√£o getTestMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b979eb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score,recall_score, f1_score, roc_auc_score\n",
    "\n",
    "def getTestMetrics(y_test, y_pred, y_prob,full_metrics=True, print_charts=False):\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    precision_neg = precision_score(y_test, y_pred, pos_label=0)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    recall_neg = recall_score(y_test, y_pred, pos_label=0)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    f1_neg = 2 * (precision_neg * recall_neg) / (precision_neg + recall_neg)\n",
    "    roc_auc = roc_auc_score(y_test, y_prob)\n",
    "    \n",
    "    return (acc, precision, precision_neg, recall, recall_neg, f1, f1_neg, roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df04daf9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ol√° TfidfVectorizer()\n",
      "Logistic Regression\n",
      "Bernoulli Naive-Bayes\n",
      "Multinomial Naive-Bayes\n",
      "Linear Support Vector Machine\n",
      "KNN\n",
      "Linear SVM with SGD training.\n",
      "Random Forest\n",
      "Gradient Boosting\n",
      "Multilayer perceptron\n",
      "Iteration 1, loss = 0.59790955\n",
      "Validation score: 0.990220\n",
      "Iteration 2, loss = 0.35856331\n",
      "Validation score: 0.990220\n",
      "Iteration 3, loss = 0.19063648\n",
      "Validation score: 0.992665\n",
      "Iteration 4, loss = 0.10533230\n",
      "Validation score: 0.992665\n",
      "Iteration 5, loss = 0.06452762\n",
      "Validation score: 0.992665\n",
      "Iteration 6, loss = 0.04398080\n",
      "Validation score: 0.992665\n",
      "ellapsed time (min): 1.1820106029510498\n",
      "../results_wpp/ml-tfidf-random_oversampling.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (6) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ol√° TfidfVectorizer()\n",
      "Logistic Regression\n",
      "Bernoulli Naive-Bayes\n",
      "Multinomial Naive-Bayes\n",
      "Linear Support Vector Machine\n",
      "KNN\n",
      "Linear SVM with SGD training.\n",
      "Random Forest\n",
      "Gradient Boosting\n",
      "Multilayer perceptron\n",
      "Iteration 1, loss = 0.60541662\n",
      "Validation score: 0.997555\n",
      "Iteration 2, loss = 0.38313919\n",
      "Validation score: 0.997555\n",
      "Iteration 3, loss = 0.21668799\n",
      "Validation score: 1.000000\n",
      "Iteration 4, loss = 0.12472366\n",
      "Validation score: 1.000000\n",
      "Iteration 5, loss = 0.07805377\n",
      "Validation score: 1.000000\n",
      "Iteration 6, loss = 0.05366427\n",
      "Validation score: 1.000000\n",
      "ellapsed time (min): 1.9305681228637694\n",
      "../results_wpp/ml-tfidf-processed-random_oversampling.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (6) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ol√° TfidfVectorizer(ngram_range=(1, 2))\n",
      "Logistic Regression\n",
      "Bernoulli Naive-Bayes\n",
      "Multinomial Naive-Bayes\n",
      "Linear Support Vector Machine\n",
      "KNN\n",
      "Linear SVM with SGD training.\n",
      "Random Forest\n",
      "Gradient Boosting\n",
      "Multilayer perceptron\n",
      "Iteration 1, loss = 0.56446112\n",
      "Validation score: 0.995110\n",
      "Iteration 2, loss = 0.28192586\n",
      "Validation score: 0.995110\n",
      "Iteration 3, loss = 0.13593276\n",
      "Validation score: 0.995110\n",
      "Iteration 4, loss = 0.07375060\n",
      "Validation score: 0.995110\n",
      "Iteration 5, loss = 0.04625023\n",
      "Validation score: 0.995110\n",
      "Iteration 6, loss = 0.03218775\n",
      "Validation score: 0.997555\n",
      "ellapsed time (min): 4.556473171710968\n",
      "../results_wpp/ml-tfidf-unibi_gram-random_oversampling.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (6) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ol√° TfidfVectorizer(ngram_range=(1, 2))\n",
      "Logistic Regression\n",
      "Bernoulli Naive-Bayes\n",
      "Multinomial Naive-Bayes\n",
      "Linear Support Vector Machine\n",
      "KNN\n",
      "Linear SVM with SGD training.\n",
      "Random Forest\n",
      "Gradient Boosting\n",
      "Multilayer perceptron\n",
      "Iteration 1, loss = 0.58845921\n",
      "Validation score: 1.000000\n",
      "Iteration 2, loss = 0.34397870\n",
      "Validation score: 1.000000\n",
      "Iteration 3, loss = 0.18979520\n",
      "Validation score: 1.000000\n",
      "Iteration 4, loss = 0.10999977\n",
      "Validation score: 1.000000\n",
      "Iteration 5, loss = 0.06891570\n",
      "Validation score: 1.000000\n",
      "Iteration 6, loss = 0.04709521\n",
      "Validation score: 1.000000\n",
      "ellapsed time (min): 4.516834922631582\n",
      "../results_wpp/ml-tfidf-unibi_gram-processed-random_oversampling.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (6) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ol√° TfidfVectorizer(ngram_range=(1, 3))\n",
      "Logistic Regression\n",
      "Bernoulli Naive-Bayes\n",
      "Multinomial Naive-Bayes\n",
      "Linear Support Vector Machine\n",
      "KNN\n",
      "Linear SVM with SGD training.\n",
      "Random Forest\n",
      "Gradient Boosting\n",
      "Multilayer perceptron\n",
      "Iteration 1, loss = 0.55475186\n",
      "Validation score: 0.975550\n",
      "Iteration 2, loss = 0.27133139\n",
      "Validation score: 0.997555\n",
      "Iteration 3, loss = 0.13633631\n",
      "Validation score: 1.000000\n",
      "Iteration 4, loss = 0.07497348\n",
      "Validation score: 1.000000\n",
      "Iteration 5, loss = 0.04697993\n",
      "Validation score: 1.000000\n",
      "Iteration 6, loss = 0.03305108\n",
      "Validation score: 1.000000\n",
      "ellapsed time (min): 9.441463712851206\n",
      "../results_wpp/ml-tfidf-unibitri_gram-random_oversampling.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (6) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ol√° TfidfVectorizer(ngram_range=(1, 3))\n",
      "Logistic Regression\n",
      "Bernoulli Naive-Bayes\n",
      "Multinomial Naive-Bayes\n",
      "Linear Support Vector Machine\n",
      "KNN\n",
      "Linear SVM with SGD training.\n",
      "Random Forest\n",
      "Gradient Boosting\n",
      "Multilayer perceptron\n",
      "Iteration 1, loss = 0.58466715\n",
      "Validation score: 0.926650\n",
      "Iteration 2, loss = 0.31724549\n",
      "Validation score: 0.995110\n",
      "Iteration 3, loss = 0.16635521\n",
      "Validation score: 1.000000\n",
      "Iteration 4, loss = 0.09373118\n",
      "Validation score: 1.000000\n",
      "Iteration 5, loss = 0.05864357\n",
      "Validation score: 1.000000\n",
      "Iteration 6, loss = 0.04098348\n",
      "Validation score: 1.000000\n",
      "ellapsed time (min): 7.604434231917064\n",
      "../results_wpp/ml-tfidf-unibitri_gram-processed-random_oversampling.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (6) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ol√° CountVectorizer(binary=True)\n",
      "Logistic Regression\n",
      "Bernoulli Naive-Bayes\n",
      "Multinomial Naive-Bayes\n",
      "Linear Support Vector Machine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN\n",
      "Linear SVM with SGD training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Gradient Boosting\n",
      "Multilayer perceptron\n",
      "Iteration 1, loss = 0.31605075\n",
      "Validation score: 0.997555\n",
      "Iteration 2, loss = 0.06595636\n",
      "Validation score: 0.997555\n",
      "Iteration 3, loss = 0.02931431\n",
      "Validation score: 0.997555\n",
      "Iteration 4, loss = 0.02064178\n",
      "Validation score: 0.987775\n",
      "Iteration 5, loss = 0.01611178\n",
      "Validation score: 0.997555\n",
      "Iteration 6, loss = 0.01201589\n",
      "Validation score: 0.997555\n",
      "ellapsed time (min): 0.9873830278714498\n",
      "../results_wpp/ml-bow-random_oversampling.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (6) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ol√° CountVectorizer(binary=True)\n",
      "Logistic Regression\n",
      "Bernoulli Naive-Bayes\n",
      "Multinomial Naive-Bayes\n",
      "Linear Support Vector Machine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN\n",
      "Linear SVM with SGD training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Gradient Boosting\n",
      "Multilayer perceptron\n",
      "Iteration 1, loss = 0.39032237\n",
      "Validation score: 0.997555\n",
      "Iteration 2, loss = 0.11769547\n",
      "Validation score: 0.997555\n",
      "Iteration 3, loss = 0.05388147\n",
      "Validation score: 0.997555\n",
      "Iteration 4, loss = 0.03302616\n",
      "Validation score: 0.997555\n",
      "Iteration 5, loss = 0.02370147\n",
      "Validation score: 0.997555\n",
      "Iteration 6, loss = 0.01885691\n",
      "Validation score: 0.997555\n",
      "ellapsed time (min): 1.7936145623524984\n",
      "../results_wpp/ml-bow-random_oversampling-processed.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (6) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ol√° CountVectorizer(binary=True, ngram_range=(1, 2))\n",
      "Logistic Regression\n",
      "Bernoulli Naive-Bayes\n",
      "Multinomial Naive-Bayes\n",
      "Linear Support Vector Machine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN\n",
      "Linear SVM with SGD training.\n",
      "Random Forest\n",
      "Gradient Boosting\n",
      "Multilayer perceptron\n",
      "Iteration 1, loss = 0.30230652\n",
      "Validation score: 0.992665\n",
      "Iteration 2, loss = 0.06618836\n",
      "Validation score: 0.992665\n",
      "Iteration 3, loss = 0.03363538\n",
      "Validation score: 0.995110\n",
      "Iteration 4, loss = 0.02001259\n",
      "Validation score: 0.995110\n",
      "Iteration 5, loss = 0.01583972\n",
      "Validation score: 0.995110\n",
      "Iteration 6, loss = 0.01150624\n",
      "Validation score: 0.995110\n",
      "ellapsed time (min): 4.317680044968923\n",
      "../results_wpp/ml-bow-unibi_gram-processed-random_oversampling.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (6) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ol√° CountVectorizer(binary=True, ngram_range=(1, 2))\n",
      "Logistic Regression\n",
      "Bernoulli Naive-Bayes\n",
      "Multinomial Naive-Bayes\n",
      "Linear Support Vector Machine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN\n",
      "Linear SVM with SGD training.\n",
      "Random Forest\n",
      "Gradient Boosting\n",
      "Multilayer perceptron\n",
      "Iteration 1, loss = 0.24183631\n",
      "Validation score: 0.995110\n",
      "Iteration 2, loss = 0.03121575\n",
      "Validation score: 0.995110\n",
      "Iteration 3, loss = 0.02030023\n",
      "Validation score: 0.995110\n",
      "Iteration 4, loss = 0.01585404\n",
      "Validation score: 0.995110\n",
      "Iteration 5, loss = 0.01659427\n",
      "Validation score: 0.995110\n",
      "Iteration 6, loss = 0.01110904\n",
      "Validation score: 0.995110\n",
      "ellapsed time (min): 4.0846235434214275\n",
      "../results_wpp/ml-bow-unibi_gram-random_oversampling.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (6) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ol√° CountVectorizer(binary=True, ngram_range=(1, 3))\n",
      "Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive-Bayes\n",
      "Multinomial Naive-Bayes\n",
      "Linear Support Vector Machine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN\n",
      "Linear SVM with SGD training.\n",
      "Random Forest\n",
      "Gradient Boosting\n",
      "Multilayer perceptron\n",
      "Iteration 1, loss = 0.22758551\n",
      "Validation score: 0.995110\n",
      "Iteration 2, loss = 0.01877473\n",
      "Validation score: 0.995110\n",
      "Iteration 3, loss = 0.00916691\n",
      "Validation score: 0.995110\n",
      "Iteration 4, loss = 0.00724669\n",
      "Validation score: 0.995110\n",
      "Iteration 5, loss = 0.00643120\n",
      "Validation score: 0.995110\n",
      "Iteration 6, loss = 0.00611246\n",
      "Validation score: 0.995110\n",
      "ellapsed time (min): 8.471950721740722\n",
      "../results_wpp/ml-bow-unibitri_gram-random_oversampling.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (6) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ol√° CountVectorizer(binary=True, ngram_range=(1, 3))\n",
      "Logistic Regression\n",
      "Bernoulli Naive-Bayes\n",
      "Multinomial Naive-Bayes\n",
      "Linear Support Vector Machine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN\n",
      "Linear SVM with SGD training.\n",
      "Random Forest\n",
      "Gradient Boosting\n",
      "Multilayer perceptron\n",
      "Iteration 1, loss = 0.31288339\n",
      "Validation score: 0.997555\n",
      "Iteration 2, loss = 0.07620583\n",
      "Validation score: 0.997555\n",
      "Iteration 3, loss = 0.04654722\n",
      "Validation score: 0.997555\n",
      "Iteration 4, loss = 0.02341725\n",
      "Validation score: 0.997555\n",
      "Iteration 5, loss = 0.01850497\n",
      "Validation score: 0.997555\n",
      "Iteration 6, loss = 0.01494433\n",
      "Validation score: 0.997555\n",
      "ellapsed time (min): 7.3617075761159265\n",
      "../results_wpp/ml-bow-unibitri_gram-processed-random_oversampling.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiago.gadelha\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (6) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for experiment in experiments:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if subset == 'viral':\n",
    "        df = df[df['viral']==1]\n",
    "        \n",
    "    if subset == 'viral-15-words':\n",
    "        df = df[df['Contador']>1]\n",
    "        df = df[df['n_words']>15]\n",
    "    \n",
    "    \n",
    "    texts = df['text_content']\n",
    "    y = df['Golpe']\n",
    "       \n",
    "    #removing duplicates\n",
    "        \n",
    "    df = df.drop_duplicates(subset=['text_content'])    \n",
    "    texts = df['text_content']\n",
    "    y = df['Golpe']\n",
    "    \n",
    "    \n",
    "    \n",
    "    # # Pre-processing\n",
    "    # * convert url in just the domain\n",
    "    # * separate emojis\n",
    "    # * punctuation\n",
    "    \n",
    "    # [Some suggestions in this work](https://github.com/miguelfzafra/Latest-News-Classifier/blob/master/0.%20Latest%20News%20Classifier/03.%20Feature%20Engineering/03.%20Feature%20Engineering.ipynb)\n",
    "    # \n",
    "    # * **Special character cleaning**\n",
    "    # \n",
    "    # * **Upcase/downcase**\n",
    "    # \n",
    "    # * **Punctuation signs** \n",
    "    # \n",
    "    # * **Possessive pronouns**\n",
    "    # \n",
    "    # * **Stemming or Lemmatization**\n",
    "    # \n",
    "    # * **Stop words**  \n",
    "    \n",
    "    #if experiment is with pre-processed text\n",
    "    if 'processed' in experiment:\n",
    "            #text was already pre-processed\n",
    "            if preprocessed:\n",
    "                if subset != 'viral':\n",
    "                    pro_texts = pickle.load(open( \"../data_wpp/processed_texts.p\", \"rb\" ))\n",
    "                else:\n",
    "                    pro_texts = pickle.load(open( \"../data_wpp/processed_texts-viral.p\", \"rb\" ))\n",
    "            else:\n",
    "                pro_texts = [preprocess(t) for t in texts]\n",
    "                if subset != 'viral':\n",
    "                    pickle.dump(pro_texts, open( \"../data_wpp/processed_texts.p\", \"wb\" ))\n",
    "                else:\n",
    "                    pickle.dump(pro_texts, open( \"../data_wpp/processed_texts-viral.p\", \"wb\" ))\n",
    "    else:\n",
    "        #only use lowercase and separates emojis and punctuation\n",
    "        pro_texts = [processEmojisPunctuation(t.lower(),remove_punct = False) for t in texts]\n",
    "    \n",
    "    # Train-test split\n",
    "    \n",
    "    #random state = 42 for reprudictibility\n",
    "    texts_train, texts_test, y_train, y_test = train_test_split(pro_texts, y, test_size=0.2, \n",
    "                                                                        stratify = y, random_state=42)\n",
    "    \n",
    "    full_texts_train, full_texts_test, y_train, y_test = train_test_split(texts, y, test_size=0.2, \n",
    "                                                                        stratify = y, random_state=42)\n",
    "    \n",
    "    # Vectorization\n",
    "    \n",
    "    max_feat = 500\n",
    "    #print(experiment)\n",
    "    #vectorizer = None\n",
    "    \n",
    "    if 'tfidf' in experiment:\n",
    "        if 'max_features' in experiment:\n",
    "            vectorizer = TfidfVectorizer(max_features = max_feat)\n",
    "        elif 'bigram' in experiment:\n",
    "            vectorizer = TfidfVectorizer(ngram_range =(2,2))\n",
    "        elif 'trigram' in experiment:\n",
    "            vectorizer = TfidfVectorizer(ngram_range =(3,3)) \n",
    "        elif 'unibi_gram' in experiment:\n",
    "            vectorizer = TfidfVectorizer(ngram_range =(1,2))\n",
    "        elif 'unibitri_gram' in experiment:\n",
    "            vectorizer = TfidfVectorizer(ngram_range =(1,3))       \n",
    "        elif 'unibitriquad_gram' in experiment:\n",
    "            vectorizer = TfidfVectorizer(ngram_range =(1,3))  \n",
    "        else:\n",
    "            vectorizer = TfidfVectorizer()\n",
    "            \n",
    "    elif 'bow' in experiment:\n",
    "        if 'max_features' in experiment:\n",
    "            vectorizer = CountVectorizer(max_features = max_feat, binary=True)\n",
    "        elif 'bigram' in experiment:\n",
    "            vectorizer = CountVectorizer(binary=True, ngram_range =(2,2))\n",
    "        elif 'trigram' in experiment:\n",
    "            vectorizer = CountVectorizer(binary=True, ngram_range =(3,3)) \n",
    "        elif 'unibi_gram' in experiment:\n",
    "            vectorizer = CountVectorizer(binary=True, ngram_range =(1,2))\n",
    "        elif 'unibitri_gram' in experiment:\n",
    "            vectorizer = CountVectorizer(binary=True, ngram_range =(1,3))\n",
    "        else:\n",
    "            vectorizer = CountVectorizer(binary=True)\n",
    "    print(\"Ol√°\", vectorizer)\n",
    "    vectorizer.fit(texts_train)   \n",
    "    X_train = vectorizer.transform(texts_train)\n",
    "    X_test = vectorizer.transform(texts_test)\n",
    "    X = vectorizer.transform(pro_texts)\n",
    "    \n",
    "    \n",
    "    if 'smote' in experiment:\n",
    "        #oversampling with SMOTE\n",
    "        sm = SMOTE(random_state = 42)\n",
    "        X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "    elif 'undersampling' in experiment:\n",
    "        rus = RandomUnderSampler(random_state = 42)\n",
    "        X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "    elif 'random_oversampling' in experiment:\n",
    "        ros = RandomOverSampler(random_state=42)\n",
    "        X_train, y_train = ros.fit_resample(X_train, y_train)\n",
    "    \n",
    "    vocab_size = X_train.shape[1]\n",
    "    \n",
    "    # Metrics\n",
    "    scenario = []\n",
    "    model = []\n",
    "    accuracy_score_list = []\n",
    "    precision_score_list = []\n",
    "    precision_score_neg_list = []\n",
    "    recall_score_list = []\n",
    "    recall_score_neg_list = []\n",
    "    f1_score_list = []\n",
    "    f1_score_neg_list = []\n",
    "    auc_score_list = []\n",
    "       \n",
    "    # ## Models training and test\n",
    "    \n",
    "    # ## Models training and test\n",
    "    \n",
    "    # In[39]:\n",
    "    \n",
    "    \n",
    "    print('Logistic Regression')\n",
    "    logreg = LogisticRegression().fit(X_train, y_train)\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    y_prob = logreg.predict_proba(X_test)[:,1]\n",
    "    model.append('logistic regression')\n",
    "    acc, precision, precision_neg, recall, recall_neg, f1, f1_neg, roc_auc = getTestMetrics(y_test, y_pred, y_prob, full_metrics = True, print_charts = False)\n",
    "    accuracy_score_list.append(acc)\n",
    "    precision_score_list.append(precision)\n",
    "    precision_score_neg_list.append(precision_neg)\n",
    "    recall_score_list.append(recall)\n",
    "    recall_score_neg_list.append(recall_neg)\n",
    "    f1_score_list.append(f1)\n",
    "    f1_score_neg_list.append(f1_neg)\n",
    "    auc_score_list.append(roc_auc)\n",
    "    \n",
    "    \n",
    "    # In[25]:\n",
    "    \n",
    "    \n",
    "    print('Bernoulli Naive-Bayes')\n",
    "    bnb = BernoulliNB().fit(X_train, y_train)\n",
    "    y_pred = bnb.predict(X_test)\n",
    "    y_prob = bnb.predict_proba(X_test)[:,1]\n",
    "    model.append('bernoulli naive-bayes')\n",
    "    acc, precision, precision_neg, recall, recall_neg, f1, f1_neg, roc_auc = getTestMetrics(y_test, y_pred, y_prob, full_metrics = True, print_charts = False)\n",
    "    accuracy_score_list.append(acc)\n",
    "    precision_score_list.append(precision)\n",
    "    precision_score_neg_list.append(precision_neg)\n",
    "    recall_score_list.append(recall)\n",
    "    recall_score_neg_list.append(recall_neg)\n",
    "    f1_score_list.append(f1)\n",
    "    f1_score_neg_list.append(f1_neg)\n",
    "    auc_score_list.append(roc_auc)\n",
    "    \n",
    "    \n",
    "    # In[40]:\n",
    "    \n",
    "    \n",
    "    print('Multinomial Naive-Bayes')\n",
    "    mnb = MultinomialNB().fit(X_train, y_train)\n",
    "    y_pred = mnb.predict(X_test)\n",
    "    y_prob = mnb.predict_proba(X_test)[:,1]\n",
    "    model.append('multinomial naive-bayes')\n",
    "    acc, precision, precision_neg, recall, recall_neg, f1, f1_neg, roc_auc = getTestMetrics(y_test, y_pred, y_prob, full_metrics = True, print_charts = False)\n",
    "    accuracy_score_list.append(acc)\n",
    "    precision_score_list.append(precision)\n",
    "    precision_score_neg_list.append(precision_neg)\n",
    "    recall_score_list.append(recall)\n",
    "    recall_score_neg_list.append(recall_neg)\n",
    "    f1_score_list.append(f1)\n",
    "    f1_score_neg_list.append(f1_neg)\n",
    "    auc_score_list.append(roc_auc)\n",
    "    \n",
    "    \n",
    "    # In[41]:\n",
    "    \n",
    "    \n",
    "    print('Linear Support Vector Machine')\n",
    "    svm = LinearSVC(dual=False).fit(X_train, y_train)\n",
    "    y_pred = svm.predict(X_test)\n",
    "    svm2 = LinearSVC()\n",
    "    clf = CalibratedClassifierCV(svm2) \n",
    "    clf.fit(X_train, y_train)\n",
    "    y_prob = clf.predict_proba(X_test)[:,1]\n",
    "    model.append('linear svm')\n",
    "    acc, precision, precision_neg, recall, recall_neg, f1, f1_neg, roc_auc = getTestMetrics(y_test, y_pred, y_prob, full_metrics = True, print_charts = False)\n",
    "    accuracy_score_list.append(acc)\n",
    "    precision_score_list.append(precision)\n",
    "    precision_score_neg_list.append(precision_neg)\n",
    "    recall_score_list.append(recall)\n",
    "    recall_score_neg_list.append(recall_neg)\n",
    "    f1_score_list.append(f1)\n",
    "    f1_score_neg_list.append(f1_neg)\n",
    "    auc_score_list.append(roc_auc)\n",
    "    \n",
    "    \n",
    "    # In[42]:\n",
    "    \n",
    "    \n",
    "    print('KNN')\n",
    "    knn = KNeighborsClassifier().fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    y_prob = knn.predict_proba(X_test)[:,1]\n",
    "    model.append('knn')\n",
    "    acc, precision, precision_neg, recall, recall_neg, f1, f1_neg, roc_auc = getTestMetrics(y_test, y_pred, y_prob, full_metrics = True, print_charts = False)\n",
    "    accuracy_score_list.append(acc)\n",
    "    precision_score_list.append(precision)\n",
    "    precision_score_neg_list.append(precision_neg)\n",
    "    recall_score_list.append(recall)\n",
    "    recall_score_neg_list.append(recall_neg)\n",
    "    f1_score_list.append(f1)\n",
    "    f1_score_neg_list.append(f1_neg)\n",
    "    auc_score_list.append(roc_auc)\n",
    "    \n",
    "    \n",
    "    # In[45]:\n",
    "    \n",
    "    \n",
    "    print('Linear SVM with SGD training.')\n",
    "    sgd = SGDClassifier().fit(X_train, y_train)\n",
    "    y_pred = sgd.predict(X_test)\n",
    "    model.append('sgd')\n",
    "    svm.fit(X_train, y_train)\n",
    "    svm2 = SGDClassifier(loss='hinge')\n",
    "    clf = CalibratedClassifierCV(svm2) \n",
    "    clf.fit(X_train, y_train)\n",
    "    y_prob = clf.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    acc, precision, precision_neg, recall, recall_neg, f1, f1_neg, roc_auc = getTestMetrics(y_test, y_pred, y_prob, full_metrics = True, print_charts = False)\n",
    "    \n",
    "    accuracy_score_list.append(acc)\n",
    "    precision_score_list.append(precision)\n",
    "    precision_score_neg_list.append(precision_neg)\n",
    "    recall_score_list.append(recall)\n",
    "    recall_score_neg_list.append(recall_neg)\n",
    "    f1_score_list.append(f1)\n",
    "    f1_score_neg_list.append(f1_neg)\n",
    "    auc_score_list.append(roc_auc)\n",
    "    \n",
    "    \n",
    "    # In[43]:\n",
    "    \n",
    "    \n",
    "    print('Random Forest')\n",
    "    rf = RandomForestClassifier().fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    y_prob = rf.predict_proba(X_test)[:,1]\n",
    "    model.append('random forest')\n",
    "    acc, precision, precision_neg, recall, recall_neg, f1, f1_neg, roc_auc = getTestMetrics(y_test, y_pred, y_prob, full_metrics = True, print_charts = False)\n",
    "    accuracy_score_list.append(acc)\n",
    "    precision_score_list.append(precision)\n",
    "    precision_score_neg_list.append(precision_neg)\n",
    "    recall_score_list.append(recall)\n",
    "    recall_score_neg_list.append(recall_neg)\n",
    "    f1_score_list.append(f1)\n",
    "    f1_score_neg_list.append(f1_neg)\n",
    "    auc_score_list.append(roc_auc)\n",
    "    \n",
    "    \n",
    "    # In[44]:\n",
    "    \n",
    "    \n",
    "    print('Gradient Boosting')\n",
    "    gb = GradientBoostingClassifier(n_estimators=200).fit(X_train, y_train)\n",
    "    y_pred = gb.predict(X_test)\n",
    "    y_prob = gb.predict_proba(X_test)[:,1]\n",
    "    model.append('gradient boosting')\n",
    "    acc, precision, precision_neg, recall, recall_neg, f1, f1_neg, roc_auc = getTestMetrics(y_test, y_pred, y_prob, full_metrics = True, print_charts = False)\n",
    "    accuracy_score_list.append(acc)\n",
    "    precision_score_list.append(precision)\n",
    "    precision_score_neg_list.append(precision_neg)\n",
    "    recall_score_list.append(recall)\n",
    "    recall_score_neg_list.append(recall_neg)\n",
    "    f1_score_list.append(f1)\n",
    "    f1_score_neg_list.append(f1_neg)\n",
    "    auc_score_list.append(roc_auc)    \n",
    "    \n",
    "    # In[46]:\n",
    "    \n",
    "    \n",
    "    print('Multilayer perceptron')\n",
    "    mlp = MLPClassifier(max_iter = 6, verbose=True, early_stopping= True).fit(X_train, y_train)\n",
    "    y_pred = mlp.predict(X_test)\n",
    "    y_prob = mlp.predict_proba(X_test)[:,1]\n",
    "    model.append('mlp')\n",
    "    acc, precision, precision_neg, recall, recall_neg, f1, f1_neg, roc_auc = getTestMetrics(y_test, y_pred, y_prob, full_metrics = True, print_charts = False)\n",
    "    accuracy_score_list.append(acc)\n",
    "    precision_score_list.append(precision)\n",
    "    precision_score_neg_list.append(precision_neg)\n",
    "    recall_score_list.append(recall)\n",
    "    recall_score_neg_list.append(recall_neg)\n",
    "    f1_score_list.append(f1)\n",
    "    f1_score_neg_list.append(f1_neg)\n",
    "    auc_score_list.append(roc_auc)\n",
    "    end_time = time.time()\n",
    "    ellapsed_time = end_time - start_time\n",
    "    print('ellapsed time (min):', ellapsed_time/60)    \n",
    "    \n",
    "    df_metrics = pd.DataFrame({'model':model,                                 \n",
    "                                     'vocab':[vocab_size]*len(model),\n",
    "                                     'auc score': auc_score_list,\n",
    "                                     'accuracy':accuracy_score_list,\n",
    "                                     'precision 1': precision_score_list,\n",
    "                                     'recall 1': recall_score_list,\n",
    "                                     'f1 score 1': f1_score_list,\n",
    "                                     'precision 0': precision_score_neg_list,\n",
    "                                     'recall 0': recall_score_neg_list,                                 \n",
    "                                     'f1 score 0': f1_score_neg_list\n",
    "                                     })\n",
    "    \n",
    "    df_metrics['precision avg'] = (df_metrics['precision 1'] + df_metrics['precision 0'])/2\n",
    "    df_metrics['recall avg'] = (df_metrics['recall 1'] + df_metrics['recall 0'])/2\n",
    "    df_metrics['f1 avg'] = (df_metrics['f1 score 1'] + df_metrics['f1 score 0'])/2\n",
    "    df_metrics.set_index('model', inplace=True)\n",
    "\n",
    "    filepath = '../results_wpp/' + experiment + '.csv'\n",
    "    print(filepath)\n",
    "    df_metrics.to_csv(filepath) \n",
    "    \n",
    "#%% update files\n",
    "#    df_update = pd.read_csv(filepath)\n",
    "#    df_update.set_index('model', inplace=True)\n",
    "#    df_update.update(df_metrics)\n",
    "#    df_update = df_update.reset_index()\n",
    "#    df_update.to_csv(filepath, index = False)    \n",
    "    \n",
    "    \n",
    "    # In[35]:\n",
    "    \n",
    "    \n",
    "    #df_metrics.to_csv(filepath, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5252d41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_statistics = pd.read_csv('../results/ml-tfidf-unibitri_gram-random_oversampling.csv')\n",
    "\n",
    "df_model_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b57acb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
